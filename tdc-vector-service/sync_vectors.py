# Permite acessar vari√°veis de ambiente (MONGO_URI, QDRANT_URL, etc.)
# evitando credenciais hardcoded no c√≥digo.
import os

# Biblioteca usada para detectar e utilizar GPU (CUDA) ou CPU.
# No nosso caso, o modelo all-MiniLM-L6-v2 roda localmente com acelera√ß√£o.
import torch

# Cliente oficial do MongoDB, respons√°vel por salvar e ler o conte√∫do bruto
# (o nosso ‚ÄúData Lake‚Äù no padr√£o RAG).
from pymongo import MongoClient

# Cliente do Qdrant, que funciona como nosso banco vetorial de alta performance.
# Ele vai armazenar SOMENTE embeddings + payload leve (IDs).
from qdrant_client import QdrantClient

# Modelos HTTP do Qdrant usados para criar cole√ß√µes, definir tamanho do vetor,
# fun√ß√£o de dist√¢ncia (COSINE) e outras configura√ß√µes do √≠ndice.
from qdrant_client.http import models

# LangChain + HuggingFace:
# Essa camada carrega modelos de embedding de forma padronizada.
# Em vez de lidar com Transformers diretamente, o LangChain simplifica a API
# e fornece m√©todos convenientes como embed_documents() e embed_query().
from langchain_huggingface import HuggingFaceEmbeddings

# Carrega automaticamente vari√°veis do arquivo .env para o ambiente.
# √â essencial para separar c√≥digo de configura√ß√£o (boa pr√°tica DevOps).
from dotenv import load_dotenv


# -----------------------------------------------------------------------------
# Carregamento de vari√°veis de ambiente (.env)
# -----------------------------------------------------------------------------
# Aqui voc√™ pode comentar sobre:
# - separar configura√ß√£o de c√≥digo (12-factor app),
# - a mesma app rodando em ambientes diferentes s√≥ mudando o .env.
load_dotenv()

# --- 1. SETUP GERAL ----------------------------------------------------------
# Detectamos automaticamente se h√° GPU dispon√≠vel (CUDA).
# Isso √© √≥timo pra demo: voc√™ mostra que a mesma aplica√ß√£o escala de
# "rodar no notebook" at√© "rodar em servidor com GPU".
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üñ•Ô∏è  Hardware: {device.upper()}")

# Carregamos o modelo de embedding all-MiniLM-L6-v2.
# Pontos importantes pra voc√™ comentar:
# - Ele √© open source, leve e r√°pido.
# - Gera vetores de 384 dimens√µes.
# - Est√° rodando LOCAL, dentro do nosso servi√ßo Python (economia de custo e lat√™ncia).
embeddings_model = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={'device': device}
)

# Constantes de conex√£o e metadados do √≠ndice
MONGO_URI = os.getenv("MONGO_URI")
QDRANT_URL = os.getenv("QDRANT_URL")
COLLECTION_NAME = "tdc_index"
# Tamanho do vetor gerado pelo all-MiniLM-L6-v2.
VECTOR_SIZE = 384

print("üîå Conectando...")
# Conex√£o com o nosso "Data Lake" de verdade: MongoDB.
mongo_client = MongoClient(MONGO_URI)
db = mongo_client["tdc_data"]

# Cliente do Qdrant: esse √© o nosso "banco vetorial", mas que funciona como √çNDICE.
qdrant_client = QdrantClient(url=QDRANT_URL)


# --- 2. FORMATA√á√ÉO ESTRAT√âGICA (TEXTOS PARA EMBEDDING) -----------------------
# Aqui entram as fun√ß√µes que transformam JSON em texto "bonito" para o modelo.

def format_event_info(info):
    """
    Fun√ß√£o de Pr√©-Processamento do Evento:

    Pega o JSON bruto do evento (nome, descri√ß√£o, local, ingressos)
    e transforma em um bloco de texto cont√≠nuo, ideal para ser vetorizado.

    Ideia chave pra explicar:
    - O modelo de embedding "entende" texto, n√£o JSON.
    - Ent√£o fazemos uma "vista textual" do objeto antes de gerar o vetor.
    """
    prices = ", ".join([f"{t['name']}: {t['price_cash']}" for t in info['tickets']['items']])
    return (
        f"EVENTO: {info['event_name']} ({info['year']})\n"
        f"DESCRI√á√ÉO: {info['description']}\n"
        f"ONDE: {info['location']['venue']}\n"
        f"INGRESSOS: {prices}"
    )


def generate_dual_vectors(talk):
    """
    üö® ESTRAT√âGIA CHAVE: DOUBLE INDEXING (Indexa√ß√£o Dupla por TEMA e por PESSOA)

    Problema real encontrado:
      - Quando colocamos tudo (nome do palestrante + tema t√©cnico) no MESMO embedding,
        a sem√¢ntica de "pessoa" e "t√≥pico" se misturam.
      - Perguntar "quem √© Rodrigo Tavares?" pode competir com "palestra sobre RAG".

    Solu√ß√£o:
      - Gerar DOIS textos diferentes para a mesma palestra:
        1) Um texto focado em tema (t√≠tulo, trilha, descri√ß√£o t√©cnica).
        2) Um texto focado em pessoa (nome, bio, cargo, LinkedIn).
      - Cada texto gera um vetor diferente.
      - Ambos os vetores apontam para o mesmo mongo_id.

    Resultado:
      - Se a pergunta falar de tecnologia ‚Üí o vetor "topic" tende a ganhar.
      - Se a pergunta for pelo nome do palestrante ‚Üí o vetor "person" tende a ganhar.
    """
    speaker = talk.get('speaker', {})
    name = speaker.get('name', 'N√£o informado')
    title = talk.get('title')
    track = talk.get('track', 'Geral')

    # 1. CONTEXTO DO TEMA (prioriza vocabul√°rio t√©cnico, trilha, resumo da talk)
    text_topic = (
        f"PALESTRA T√âCNICA: {title}\n"
        f"TRILHA: {track}\n"
        f"RESUMO: {talk.get('description')}\n"
        f"N√çVEL: {talk.get('level', 'T√©cnico')}"
    )

    # 2. CONTEXTO DO PALESTRANTE (prioriza nome, bio, cargo, LinkedIn)
    text_person = (
        f"QUEM √â O PALESTRANTE: {name}\n"
        f"BIO/ROLE: {speaker.get('role', '')}\n"
        f"APRESENTA A PALESTRA: {title}\n"
        f"LINKEDIN: {speaker.get('linkedin', '')}"
    )

    # Retornamos os dois textos + o nome do palestrante (para payload).
    return text_topic, text_person, name


# --- 3. EXECU√á√ÉO PRINCIPAL ---------------------------------------------------
def main():
    print("üöÄ Iniciando Sync com Estrat√©gia 'Double Indexing' (somente √≠ndices no Qdrant)...")

    # -------------------------------------------------------------------------
    # 3.1. Limpeza/Recria√ß√£o da Cole√ß√£o
    # -------------------------------------------------------------------------
    # Esse passo garante um √≠ndice "limpo":
    # - se a cole√ß√£o j√° existe, apagamos;
    # - depois criamos de novo com a configura√ß√£o correta.
    # Em termos de processo, isso √© um "full reindex" completo do vetor.
    if qdrant_client.collection_exists(COLLECTION_NAME):
        qdrant_client.delete_collection(COLLECTION_NAME)

    # Cria a collection no Qdrant:
    # - VECTOR_SIZE: dimens√£o do embedding (384).
    # - Distance.COSINE: m√©trica de similaridade que vamos usar na busca.
    qdrant_client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=models.VectorParams(size=VECTOR_SIZE, distance=models.Distance.COSINE)
    )

    # Essas duas listas trabalham em paralelo:
    # - documents_text[i] ‚Üí texto que vai virar embedding.
    # - payloads[i]       ‚Üí metadado associado ao vetor gerado a partir daquele texto.
    documents_text = []
    payloads = []

    # -------------------------------------------------------------------------
    # 3.2. Indexa√ß√£o da Info Geral do Evento
    # -------------------------------------------------------------------------
    # Aqui o RAG aprende coisas como:
    # - descri√ß√£o do TDC,
    # - local,
    # - tipos de ingressos e pre√ßos.
    event_info = db.event_info.find_one()
    if event_info:
        # Converte o JSON em texto
        text = format_event_info(event_info)
        # Guarda o texto para virar embedding depois
        documents_text.append(text)

        # E aqui est√° a parte importante do padr√£o RAG Enterprise:
        # O payload √© LEVE. Guardamos APENAS refer√™ncia:
        # - mongo_id: para buscar o documento de volta no Mongo.
        # - type: para saber se √© info de evento ou talk.
        # - title: s√≥ pra fins de debug/exibi√ß√£o.
        payloads.append({
            "mongo_id": str(event_info['_id']),
            "type": "event_info",
            "title": "Informa√ß√µes do Evento"
        })

    # -------------------------------------------------------------------------
    # 3.3. Indexa√ß√£o das Palestras (Double Indexing)
    # -------------------------------------------------------------------------
    # Vamos carregar todas as talks do Mongo (Data Lake).
    talks = list(db.talks.find({}))
    print(f"üì¶ Processando {len(talks)} palestras (gerando 2 vetores por palestra)...")

    for talk in talks:
        # Gera os dois textos: um para o tema, outro para a pessoa.
        txt_topic, txt_person, speaker_name = generate_dual_vectors(talk)

        # Payload base com o ID do Mongo.
        # ESSENCIAL: √© esse ID que ser√° usado depois para "hidratar" o contexto
        # indo buscar o texto completo no Mongo na hora da pergunta.
        base_payload = {
            "mongo_id": str(talk['_id']),
            "type": "talk",
            "title": talk.get('title'),
            "speaker": speaker_name
        }

        # VETOR 1: TEMA (vector_type = topic)
        documents_text.append(txt_topic)
        payload1 = base_payload.copy()
        payload1["vector_type"] = "topic"  # √∫til para debug e an√°lise de relev√¢ncia
        payloads.append(payload1)

        # VETOR 2: PESSOA (vector_type = person)
        documents_text.append(txt_person)
        payload2 = base_payload.copy()
        payload2["vector_type"] = "person"  # idem: ajuda a entender de onde veio o match
        payloads.append(payload2)

    # -------------------------------------------------------------------------
    # 3.4. Gera√ß√£o de Embeddings em Lote
    # -------------------------------------------------------------------------
    # Agora, com todos os textos consolidados, mandamos gerar os vetores.
    # Isso √© mais eficiente do que chamar o modelo um por um.
    print(f"üß† Gerando Embeddings para {len(documents_text)} vetores em {device.upper()}...")
    vectors = embeddings_model.embed_documents(documents_text)

    # -------------------------------------------------------------------------
    # 3.5. Upload para o Qdrant (√çndice Vetorial)
    # -------------------------------------------------------------------------
    # Aqui acontece a "m√°gica" da indexa√ß√£o:
    # - Cada vetor gerado vai para o Qdrant,
    # - Acompanhado do payload leve (com mongo_id, type, title, vector_type...).
    # Importante refor√ßar na fala:
    #   ‚ùå N√ÉO estamos salvando o texto completo no Qdrant.
    #   ‚úÖ SOMENTE embeddings + IDs ‚Üí Mongo continua sendo a fonte de verdade.
    print(f"üíæ Salvando {len(vectors)} pontos no Qdrant (somente √≠ndices, sem conte√∫do bruto)...")
    qdrant_client.upload_collection(
        collection_name=COLLECTION_NAME,
        vectors=vectors,
        payload=payloads
    )

    print("‚úÖ Sincroniza√ß√£o Finalizada! Qdrant agora guarda apenas embeddings + IDs (Mongo como fonte de verdade).")


# Ponto de entrada do script.
# Na narrativa de arquitetura:
# - isso aqui poderia ser um job agendado (cron, Azure Functions Timer Trigger,
#   GitHub Actions, Azure DevOps Pipeline, etc.).
if __name__ == "__main__":
    main()
