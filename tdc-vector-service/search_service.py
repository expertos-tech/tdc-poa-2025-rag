# M√≥dulo padr√£o do Python para acessar vari√°veis de ambiente (ex: MONGO_URI, QDRANT_URL, etc.)
import os

# Usado para medir tempos de execu√ß√£o (lat√™ncia de embedding, Qdrant, Mongo, GPT, etc.)
import time

# Biblioteca para detectar e usar GPU (CUDA) ou CPU ao rodar o modelo de embeddings
import torch

# Framework web ass√≠ncrono que exp√µe nossa API HTTP (/ask, /debug/search)
from fastapi import FastAPI

# Middleware para habilitar CORS (permitir que o frontend em outro dom√≠nio/porta acesse essa API)
from fastapi.middleware.cors import CORSMiddleware

# Base para definir modelos de request/response tipados (SearchRequest, SearchResponse, etc.)
from pydantic import BaseModel

# Cliente oficial do MongoDB em Python, usado para conectar e consultar o nosso ‚ÄúData Lake‚Äù
from pymongo import MongoClient

# Classe para trabalhar com IDs do Mongo (_id), convertendo strings em ObjectId e vice-versa
from bson.objectid import ObjectId

# Cliente do Qdrant, o banco vetorial usado como √≠ndice de similaridade sem√¢ntica
from qdrant_client import QdrantClient

# Wrapper do LangChain para carregar o modelo de embeddings HuggingFace (all-MiniLM-L6-v2)
from langchain_huggingface import HuggingFaceEmbeddings

# Wrapper do LangChain para se conectar ao Azure OpenAI (GPT) via deployment configurado
from langchain_openai import AzureChatOpenAI

# Tipos de mensagens usados para montar o prompt de chat (System + Human) para o GPT
from langchain_core.messages import SystemMessage, HumanMessage

# Facilita o carregamento de vari√°veis de ambiente a partir de um arquivo .env
from dotenv import load_dotenv

# =============================================================================
# 1. CONFIGURA√á√ÉO INICIAL
# =============================================================================
# Esse arquivo √© essencialmente o "orquestrador" do RAG:
# - recebe a pergunta do frontend
# - vetoriza localmente
# - busca no Qdrant
# - hidrata no Mongo
# - monta contexto
# - chama Azure OpenAI
# - devolve resposta e fontes para o frontend
# Tudo isso numa API HTTP simples, via FastAPI.

load_dotenv()

app = FastAPI(title="TDC RAG Search Service", version="1.1.0")

# -------------------------------------------------------------------------
# CORS liberado (para demo)
# -------------------------------------------------------------------------
# Durante a palestra / prot√≥tipo, √© muito mais simples liberar tudo.
# Em produ√ß√£o:
#   - aqui voc√™ restringiria domains espec√≠ficos do front
#   - controlaria m√©todos e headers aceita dos.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],      # em prod ‚Üí ["https://meu-front.com", ...]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# -------------------------------------------------------------------------
# Hardware para embeddings
# -------------------------------------------------------------------------
# Mesma l√≥gica dos outros scripts:
# - se tiver GPU (CUDA), usa GPU
# - se n√£o tiver, cai pra CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üöÄ API Iniciada | Hardware de Vetoriza√ß√£o: {device.upper()}")

# -------------------------------------------------------------------------
# Modelo de embeddings local (all-MiniLM-L6-v2)
# -------------------------------------------------------------------------
# Esse √© o "c√©rebro sem√¢ntico" da parte de busca vetorial.
# Ponto importante pra explicar:
# - N√£o estamos usando GPT para embeddings aqui.
# - O modelo √© local, open source, r√°pido e barato.
print("üì• Carregando modelo de vetores (Local all-MiniLM-L6-v2)...")
embeddings_model = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={'device': device}
)

# -------------------------------------------------------------------------
# Cliente do Azure OpenAI (Texto ‚Üí Resposta final)
# -------------------------------------------------------------------------
# Esse √© o modelo "grande" (GPT) que:
# - l√™ o contexto vindo do Mongo
# - responde a pergunta do usu√°rio
#
# Observa√ß√£o importante:
# - N√£o setamos temperature porque esse deployment espec√≠fico
#   n√£o aceita override (erro 400 se fizer isso).
print(f"ü§ñ Conectando ao Azure OpenAI ({os.getenv('AZURE_DEPLOYMENT_NAME')})...")
llm = AzureChatOpenAI(
    azure_deployment=os.getenv("AZURE_DEPLOYMENT_NAME"),
    api_version=os.getenv("OPENAI_API_VERSION"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    max_tokens=800,
    # importante: n√£o setar temperature aqui, o modelo do deployment n√£o aceita
)

# -------------------------------------------------------------------------
# MongoDB: Data Lake / Fonte de Verdade
# -------------------------------------------------------------------------
# - Aqui est√£o os documentos COMPLETOS.
# - Toda a hidrata√ß√£o do RAG acontece consultando esse banco.
mongo_client = MongoClient(os.getenv("MONGO_URI"))
db = mongo_client["tdc_data"]

# -------------------------------------------------------------------------
# Qdrant: √çndice Vetorial
# -------------------------------------------------------------------------
# - Aqui s√≥ vivem embeddings + payload leve (IDs).
# - Servidor de similaridade sem√¢ntica.
qdrant_client = QdrantClient(url=os.getenv("QDRANT_URL"))
COLLECTION_INDEX = "tdc_index"


# =============================================================================
# 2. MODELOS Pydantic (Request/Response)
# =============================================================================

class SearchRequest(BaseModel):
    """
    Payload de entrada padr√£o da API:
    - text: pergunta do usu√°rio
    - limit: quantos documentos √∫nicos (do Mongo) queremos para montar o contexto
    """
    text: str
    limit: int = 5


class SearchResponse(BaseModel):
    """
    Resposta final para o frontend:
    - answer: texto gerado pelo GPT (j√° em markdown)
    - sources: lista de t√≠tulos/fontes que alimentaram o contexto
    - time_taken: tempo total da opera√ß√£o
    - timings: tempos detalhados por etapa (embedding, qdrant, mongo, gpt)
    """
    answer: str
    sources: list[str]
    time_taken: float
    timings: dict[str, float] | None = None  # tempos por etapa em milissegundos


class DebugHit(BaseModel):
    """
    Modelo de cada hit para o endpoint de debug:
    - mongo_id: refer√™ncia pro documento real no Mongo
    - title: t√≠tulo da talk ou nome do evento
    - type: talk ou event_info
    - vector_type: topic/person/single (√∫til pra explicar o Double Indexing)
    - score: score de similaridade retornado pelo Qdrant
    """
    mongo_id: str
    title: str | None = None
    type: str | None = None
    vector_type: str | None = None
    score: float


class DebugSearchResponse(BaseModel):
    """
    Resposta do endpoint /debug/search:
    - query: pergunta original
    - limit: limite pedido
    - hits: lista deduplicada de resultados do Qdrant
    """
    query: str
    limit: int
    hits: list[DebugHit]


# =============================================================================
# 3. FUN√á√ÉO AUXILIAR: CONSTRU√á√ÉO DO CONTEXTO
# =============================================================================

def build_context(docs):
    """
    Monta um contexto leg√≠vel a partir dos documentos do Mongo.

    Ponto central do RAG Enterprise:
    - O contexto √© sempre montado a partir do Data Lake (MongoDB),
      nunca do texto armazenado no Qdrant.
    - O Qdrant √© s√≥ um √≠ndice para achar quais documentos s√£o relevantes.

    Essa fun√ß√£o:
    - formata dados de event_info e talks
    - gera um "text√£o" bem estruturado pro GPT consumir.
    """
    context = ""
    for doc in docs:
        # Caso: documento de informa√ß√µes gerais do evento
        if "event_name" in doc:
            context += f"--- DADOS GERAIS DO EVENTO ---\n"
            context += f"Evento: {doc['event_name']} ({doc.get('year', '')})\n"

            location = doc.get("location", {})
            context += f"Local: {location.get('venue', '')} ({location.get('address', '')})\n"

            tickets = doc.get("tickets", {}).get("items", [])
            prices = ", ".join([f"{i['name']}: {i['price_cash']}" for i in tickets])
            context += f"Ingressos: {prices}\n"

            policies = doc.get("policies", {})
            if "cancellation" in policies:
                context += f"Pol√≠tica de cancelamento: {policies['cancellation']}\n"
            context += "\n"

        # Caso: documento de talk (palestra/atividade)
        elif "title" in doc:
            speaker = doc.get("speaker", {})
            context += f"--- ATIVIDADE ---\n"
            context += f"T√≠tulo: {doc['title']}\n"
            context += f"Tipo: {doc.get('type', '').upper()} | Trilha: {doc.get('track', '')}\n"
            context += f"Palestrante: {speaker.get('name', 'N√£o informado')}\n"
            context += f"Data/Hora: {doc.get('date', '')} √†s {doc.get('time', '')}\n"
            context += f"Descri√ß√£o: {doc.get('description', '')}\n\n"

    return context


# =============================================================================
# 4. ENDPOINT DE DEBUG: /debug/search (SEM GPT, SEM MONGO)
# =============================================================================

@app.post("/debug/search", response_model=DebugSearchResponse)
async def debug_search(request: SearchRequest):
    """
    Endpoint para inspecionar diretamente o resultado do Qdrant:

    O que ele faz:
    - Vetoriza a query.
    - Consulta o Qdrant.
    - Deduplica por mongo_id.
    - Retorna apenas infos b√°sicas (sem chamar Mongo, sem GPT).

    Por que isso √© √≥timo pra palestra:
    - D√° pra mostrar ao vivo:
      - os scores,
      - o tipo do documento,
      - se veio por vetor "topic" ou "person",
      - como o Double Indexing est√° se comportando.
    """
    print(f"\nüß™ [DEBUG] Buscando no Qdrant para a query: {request.text!r}")

    # 1) Medi√ß√£o do tempo de embedding
    t0 = time.perf_counter()
    query_vector = embeddings_model.embed_query(request.text)
    t1 = time.perf_counter()

    # 2) Busca no Qdrant com um limite um pouco maior (para deduplicar depois)
    qdrant_limit = request.limit * 2
    search = qdrant_client.query_points(
        collection_name=COLLECTION_INDEX,
        query=query_vector,
        limit=qdrant_limit,
        score_threshold=0.0  # sem filtro de score, voc√™ v√™ tudo e decide no front
    )
    t2 = time.perf_counter()

    hits_model: list[DebugHit] = []
    seen_ids: set[str] = set()

    # 3) Deduplica√ß√£o por mongo_id
    for hit in search.points:
        payload = hit.payload or {}
        mongo_id = payload.get("mongo_id")
        if not mongo_id:
            continue

        if mongo_id in seen_ids:
            continue
        seen_ids.add(mongo_id)

        # Constr√≥i o modelo de resposta amig√°vel pro front / demo
        hits_model.append(
            DebugHit(
                mongo_id=mongo_id,
                title=payload.get("title"),
                type=payload.get("type"),
                vector_type=payload.get("vector_type"),
                score=hit.score,
            )
        )

        if len(hits_model) >= request.limit:
            break

    embed_ms = (t1 - t0) * 1000
    qdrant_ms = (t2 - t1) * 1000
    print(f"‚è±Ô∏è [DEBUG] embedding={embed_ms:.2f}ms | qdrant={qdrant_ms:.2f}ms | hits={len(hits_model)}")

    return DebugSearchResponse(
        query=request.text,
        limit=request.limit,
        hits=hits_model,
    )


# =============================================================================
# 5. ENDPOINT PRINCIPAL: /ask (FLUXO COMPLETO DE RAG)
# =============================================================================

@app.post("/ask", response_model=SearchResponse)
async def ask_endpoint(request: SearchRequest):
    """
    Fluxo completo de RAG:

    1. Vetoriza a pergunta (modelo local all-MiniLM-L6-v2).
    2. Busca semelhante no Qdrant (traz s√≥ IDs + payload leve).
    3. Deduplica IDs e separa por tipo (talk vs event_info).
    4. Hidrata dados completos no MongoDB.
    5. Monta o contexto em texto.
    6. Chama Azure OpenAI (GPT) com contexto + pergunta.
    7. Retorna resposta final + fontes + tempos detalhados.
    """
    start = time.perf_counter()
    query = request.text
    print(f"\nüí¨ Pergunta: {query}")

    timings: dict[str, float] = {}

    # ---------------------------------------------------------------------
    # 1) Vetoriza√ß√£o da pergunta
    # ---------------------------------------------------------------------
    t0 = time.perf_counter()
    query_vector = embeddings_model.embed_query(query)
    t1 = time.perf_counter()
    timings["embedding_ms"] = (t1 - t0) * 1000

    # ---------------------------------------------------------------------
    # 2) Busca no Qdrant (apenas IDs + metadados)
    # ---------------------------------------------------------------------
    # Multiplica o limit por 2 pra dar espa√ßo para deduplica√ß√£o.
    qdrant_limit = request.limit * 2
    t2 = time.perf_counter()
    search = qdrant_client.query_points(
        collection_name=COLLECTION_INDEX,
        query=query_vector,
        limit=qdrant_limit,
        score_threshold=0.5  # threshold ajust√°vel; mais alto = mais estrito
    )
    t3 = time.perf_counter()
    timings["qdrant_ms"] = (t3 - t2) * 1000

    hits = search.points

    if not hits:
        total = (time.perf_counter() - start) * 1000
        print("‚ö†Ô∏è Nenhum resultado vetorial encontrado no Qdrant.")
        return SearchResponse(
            answer="N√£o encontrei informa√ß√µes relevantes no √≠ndice para responder sua pergunta.",
            sources=[],
            time_taken=total / 1000.0,
            timings=timings
        )

    # ---------------------------------------------------------------------
    # 3) Deduplica√ß√£o de IDs e separa√ß√£o por tipo
    # ---------------------------------------------------------------------
    seen_ids: set[str] = set()
    talk_ids: list[ObjectId] = []
    info_ids: list[ObjectId] = []

    print("üß† Resultados do Qdrant (antes da deduplica√ß√£o):")
    for hit in hits:
        payload = hit.payload or {}
        mongo_id = payload.get("mongo_id")
        if not mongo_id:
            continue

        # evita usar o mesmo documento duas vezes no contexto
        if mongo_id in seen_ids:
            continue

        seen_ids.add(mongo_id)

        p_type = payload.get("type")
        vector_type = payload.get("vector_type", "single")

        # Log amig√°vel pra voc√™ mostrar no terminal durante a demo
        print(
            f"   ‚Ä¢ {payload.get('title', 'Sem t√≠tulo')} "
            f"(type={p_type}, via vetor={vector_type}, score={hit.score:.3f})"
        )

        oid = ObjectId(mongo_id)
        if p_type == "event_info":
            info_ids.append(oid)
        elif p_type == "talk":
            talk_ids.append(oid)

        # respeita o limite de documentos √∫nicos para o contexto
        if len(seen_ids) >= request.limit:
            break

    # ---------------------------------------------------------------------
    # 4) Hidrata√ß√£o no MongoDB
    # ---------------------------------------------------------------------
    # Agora, com os IDs na m√£o, buscamos o conte√∫do completo no Mongo.
    t4 = time.perf_counter()
    docs = []

    if info_ids:
        docs.extend(list(db.event_info.find({"_id": {"$in": info_ids}})))

    if talk_ids:
        docs.extend(list(db.talks.find({"_id": {"$in": talk_ids}})))

    t5 = time.perf_counter()
    timings["mongo_ms"] = (t5 - t4) * 1000

    if not docs:
        total = (time.perf_counter() - start) * 1000
        print("‚ö†Ô∏è Qdrant retornou IDs, mas n√£o encontrei documentos no Mongo.")
        return SearchResponse(
            answer="N√£o consegui localizar os detalhes dessas informa√ß√µes no banco de dados.",
            sources=[],
            time_taken=total / 1000.0,
            timings=timings
        )

    # ---------------------------------------------------------------------
    # 5) Constru√ß√£o do contexto em texto
    # ---------------------------------------------------------------------
    context_str = build_context(docs)

    # Prompt de sistema:
    # - define o papel do modelo
    # - refor√ßa o idioma
    # - refor√ßa a regra de usar s√≥ o contexto
    system_prompt = """
    INSTRUCTIONS
    ===
    - Voc√™ √© o Assistente Oficial do TDC Experience.
    - MUITO IMPORTANTE:
      - Responda SEMPRE no mesmo idioma da pergunta, de forma clara e direta.
      - Se a pergunta estiver em outro idioma que n√£o seja Portugu√™s,
        traduza mentalmente as informa√ß√µes do evento para o mesmo idioma antes de responder.
    - Use APENAS o contexto fornecido abaixo. Se a informa√ß√£o n√£o estiver no contexto,
      diga que n√£o possui dados suficientes para responder.
    - Use o formato Markdown para as respostas.
    """

    messages = [
        SystemMessage(content=system_prompt),
        SystemMessage(content=f"CONTEXTO RECUPERADO:\n{context_str}"),
        HumanMessage(content=query)
    ]

    # ---------------------------------------------------------------------
    # 6) Gera√ß√£o com Azure OpenAI (GPT)
    # ---------------------------------------------------------------------
    print("ü§ñ Gerando resposta com Azure OpenAI...")
    t6 = time.perf_counter()
    ai_response = llm.invoke(messages)
    t7 = time.perf_counter()
    timings["gpt_ms"] = (t7 - t6) * 1000

    total = (time.perf_counter() - start) * 1000
    print(
        f"‚úÖ Resposta gerada em {total:.2f}ms | "
        f"embedding={timings['embedding_ms']:.2f}ms | "
        f"qdrant={timings['qdrant_ms']:.2f}ms | "
        f"mongo={timings['mongo_ms']:.2f}ms | "
        f"gpt={timings['gpt_ms']:.2f}ms"
    )

    # ---------------------------------------------------------------------
    # 7) Fontes (para exibir no frontend)
    # ---------------------------------------------------------------------
    # Aqui voc√™ extrai nomes amig√°veis dos documentos usados:
    # - t√≠tulo das talks
    # - nome do evento
    sources: list[str] = []
    for d in docs:
        if "title" in d:
            sources.append(d["title"])
        elif "event_name" in d:
            sources.append(d["event_name"])
        else:
            sources.append("Fonte desconhecida")

    return SearchResponse(
        answer=ai_response.content,
        sources=sources,
        time_taken=total / 1000.0,
        timings=timings
    )
